# Configuration for the prometheus-operator chart.
# Reference:
# https://github.com/prometheus-community/helm-charts/blob/kube-prometheus-stack-15.4.6/charts/kube-prometheus-stack/values.yaml
#
# WARNING: the prometheus-operator chart is complicated and error-prone. If you
# edit this file, run the following command to generate the output with `helm
# template`, and verify that your changes have the expected effect.
#
#   bazel build src/app_charts/prometheus/prometheus-operator-chart.robot.yaml

nameOverride: kube
fullnameOverride: kube

kubeTargetVersionOverride: "1.23.8"

alertmanager:
  enabled: false

defaultRules:
  create: false

prometheus:
  prometheusSpec:
    # Pick up all service monitors across all namespaces.
    serviceMonitorNamespaceSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"
    serviceMonitorSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"

    # Pick up all pod monitors across all namespaces.
    podMonitorNamespaceSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"
    podMonitorSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"

    # Pick up all rules across all namespaces.
    ruleNamespaceSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"
    ruleSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"

    logLevel: warn
    # Historical data is limited to the lower of `retention` and
    # `retentionSize`. WAL+maxChunkSize also counted toward retentionSize,
    # but Prometheus puts no limit on WAL size, so it's up to us to make
    # sure we don't have too many metrics and the WAL doesn't grow too big.
    retention: "3h"
    # If you increase retentionSize, increase sizeLimit below as well, but
    # remember that this is RAM, not disk. We don't know how much headroom
    # is needed, but if set equal you can still run out of disk space.
    retentionSize: 448MB
    # Reduce the max chunk size (default=512MB) to reduce the headroom required
    # for the in-progress chunk.
    # This chunk size correlates with the current retention size. Larger
    # retention sizes make it so that WAL is not truncated as it should be.
    additionalArgs:
      - name: storage.tsdb.max-block-chunk-segment-size
        value: 16MB
      - name: storage.tsdb.wal-segment-size
        value: 16MB
    storageSpec:
      emptyDir:
        medium: Memory
        sizeLimit: 512Mi
  serviceMonitor:
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

# Throws an invalid namespace "kube-system" error during deployment, as this is
# trying to install resources into the kube-system namespace, which synk does
# not support.
kubeEtcd:
  enabled: false
kubeControllerManager:
  enabled: false
kubeProxy:
  enabled: false
kubeScheduler:
  enabled: false
coreDns:
  enabled: false

prometheusOperator:
  admissionWebhooks:
    enabled: true
    certManager:
      enabled: true
      issuerRef:
        name: "selfsigned-issuer"
        kind: "ClusterIssuer"
  serviceMonitor:
    metricRelabelings:
    # Drop high cardinality kube state metrics
    - action: drop
      regex: "kube_*"
      sourceLabels: [ __name__ ]
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

# Default scraping interval is 20s and these metrics result in a large amount of data
kubeApiServer:
  serviceMonitor:
    interval: 10m
    metricRelabelings:
    # Drop high cardinality apiserver metrics.
    - action: drop
      regex: "apiserver_request.*|etcd_request.*|apiserver_watch.*|code_*"
      sourceLabels: [__name__]
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance
kubelet:
  serviceMonitor:
    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
    resourcePath: "/metrics/resource"
    metricRelabelings:
    # Drop high cardinality metrics from kubelet.
    - action: drop
      regex: "container_network.*|container_blkio.*|kubelet_.*|kubernetes_feature_enabled|container_fs.*|container_processes|container_last_seen|storage_operation_duration_seconds_bucket"
      sourceLabels: [__name__]
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance
    - sourceLabels: [__metrics_path__]
      targetLabel: metrics_path

# Subcharts

nodeExporter:
  enabled: true
  serviceMonitor:
    metricRelabelings:
    # Drop high cardinality metrics from node exporter.
    - action: drop
      regex: "node_cpu_seconds_total"
      sourceLabels: [ __name__ ]
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

prometheus-node-exporter:
  extraArgs:
    # This collector produces log-spam on newer kernels
    # https://github.com/prometheus/node_exporter/issues/1892
    - --no-collector.rapl
    # This is disabled by default, since it might leak memory
    # (https://github.com/prometheus/node_exporter/blob/master/CHANGELOG.md#0160-rc1--2018-04-04)
    - --collector.wifi
    # Ignore more fuse filesystems
    # https://github.com/prometheus/node_exporter/blob/master/collector/filesystem_linux.go#L33
    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|fuse\.\w*|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$

grafana:
  enabled: false
