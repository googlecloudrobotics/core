# Configuration for the prometheus-operator chart.
# Reference: 
# https://github.com/prometheus-community/helm-charts/blob/kube-prometheus-stack-41.5.1/charts/kube-prometheus-stack/values.yaml
#
# WARNING: the prometheus-operator chart is complicated and error-prone. If you
# edit this file, run the following command to generate the output with `helm
# template`, and verify that your changes have the expected effect.
#
#   bazel build src/app_charts/prometheus/prometheus-operator-chart.cloud.yaml

nameOverride: kube
fullnameOverride: kube

kubeTargetVersionOverride: "1.23.8"

# Alertmanagers have to be deployed individually by users.
alertmanager:
  enabled: false

defaultRules:
  rules:
    kubeApiserver: false

prometheus:
  prometheusSpec:
    # Pick up all service monitors across all namespaces.
    serviceMonitorNamespaceSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"
    serviceMonitorSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"

    # Pick up all pod monitors across all namespaces.
    podMonitorNamespaceSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"
    podMonitorSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"

    # Pick up all rules across all namespaces.
    ruleNamespaceSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"
    ruleSelector:
      # Inverse selector selects everything
      matchExpressions:
      - key: "non-existent-label-for-universal-matching"
        operator: "DoesNotExist"

    externalUrl: "${EXTERNAL_URL}"
    retention: "${RETENTION_TIME}"
    retentionSize: "${RETENTION_SIZE}"
    walCompression: true
    resources:
      requests:
        cpu: "${LIMITS_CPU}"
        memory: "${LIMITS_MEMORY}"
      limits:
        cpu: "${LIMITS_CPU}"
        memory: "${LIMITS_MEMORY}"
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: "${REQUESTS_STORAGE}"
    # Pick up user-created Alertmanager pods with app=alertmanager and a non-empty port.
    additionalAlertManagerConfigs:
    - kubernetes_sd_configs:
      - role: service
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      authorization:
        type: Bearer
        credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_label_app]
        regex: kube-alertmanager
        action: keep
    # Set absurdly high thresholds as a workaround for not being able to disable these and not having enough time to WAL replay
    # https://github.com/prometheus-operator/prometheus-operator/issues/3587
    containers:
    - name: prometheus
      readinessProbe:
        initialDelaySeconds: 300
        failureThreshold: 1000
  serviceMonitor:
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

# etcd, scheduler, and controller-manager are managed by GKE and hidden.
kubeEtcd:
  enabled: false
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
coreDns:
  enabled: false

# Throws an invalid namespace "kube-system" error during deployment, as this is
# trying to install resources into the kube-system namespace, which synk does
# not support.
kubeProxy:
  enabled: false

prometheusOperator:
  admissionWebhooks:
    enabled: true
    certManager:
      enabled: true
      issuerRef:
        name: "selfsigned-issuer"
        kind: "ClusterIssuer"
  serviceMonitor:
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance


# Default scraping interval is 20s and these metrics result in a large amount of data
kubeApiServer:
  serviceMonitor:
    interval: 1m
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

kubelet:
  serviceMonitor:
    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
    resourcePath: "/metrics/resource"
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance
    - sourceLabels: [__metrics_path__]
      targetLabel: metrics_path

kubeStateMetrics:
  serviceMonitor:
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

# Subcharts

nodeExporter:
  enabled: true
  serviceMonitor:
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

grafana:
  env:
    GF_SERVER_DOMAIN: "${CR_GF_SERVER_DOMAIN}"
    GF_SERVER_ROOT_URL: "${CR_GF_SERVER_ROOT_URL}"
    GF_AUTH_ANONYMOUS_ENABLED: "true"
  # Load dashboards from configmaps with a given label across all namespaces.
  sidecar:
    dashboards:
      enabled: true
      label: grafana # Label our own legacy grafana-operator uses.
      searchNamespace: ALL
      multicluster:
        global:
          enabled: true
        etcd:
          enabled: true
  grafana.ini:
    analytics:
      check_for_updates: false
    security:
      csrf_trusted_origins: "${CR_GF_CSRF_TRUSTED_ORIGINS}"
  serviceMonitor:
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: instance

gf_ingress_auth_url: "${CR_GF_INGRESS_AUTH_URL}"
gf_ingress_auth_signin: "${CR_GF_INGRESS_AUTH_SIGNIN}"
gf_ingress_error_page_403: "${CR_GF_INGRESS_ERROR_PAGE_403}"

prom_ingress_auth_url: "${CR_PROM_INGRESS_AUTH_URL}"
prom_ingress_auth_signin: "${CR_PROM_INGRESS_AUTH_SIGNIN}"
prom_ingress_error_page_403: "${CR_PROM_INGRESS_ERROR_PAGE_403}"
